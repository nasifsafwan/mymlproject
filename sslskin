{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9712342,"sourceType":"datasetVersion","datasetId":5939901}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, balanced_accuracy_score, confusion_matrix\nfrom torch.nn.functional import softmax\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T06:15:46.572367Z","iopub.execute_input":"2024-10-25T06:15:46.572798Z","iopub.status.idle":"2024-10-25T06:15:46.580871Z","shell.execute_reply.started":"2024-10-25T06:15:46.572759Z","shell.execute_reply":"2024-10-25T06:15:46.579751Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/isic2019' \n\nimport os\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass ISIC2019Dataset(Dataset):\n    def __init__(self, image_dir, metadata, ground_truth_csv=None, transform=None, is_train=True):\n        self.image_dir = image_dir\n        self.transform = transform\n        self.is_train = is_train  \n\n        if self.is_train:\n            self.ground_truth = pd.read_csv(ground_truth_csv)\n            self.image_paths, self.labels = self.process_images(image_dir, metadata)\n        else:\n            self.image_paths = self.process_images_without_labels(image_dir, metadata)\n\n    def process_images(self, image_dir, metadata):\n        images = []\n        labels = []\n        merged_df = pd.merge(metadata, self.ground_truth, on='image', how='inner')\n\n        for _, row in merged_df.iterrows():\n            img_path = os.path.join(image_dir, row['image'] + '.jpg')\n            if os.path.exists(img_path):\n                images.append(img_path)\n                labels.append(row[['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF']].values.astype(float))\n\n        return images, labels\n\n    def process_images_without_labels(self, image_dir, metadata):\n        images = []\n        for _, row in metadata.iterrows():\n            img_path = os.path.join(image_dir, row['image'] + '.jpg')\n            if os.path.exists(img_path):\n                images.append(img_path)\n        return images\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        if self.is_train:\n            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n            return image, label\n        else:\n            return image","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:42:04.931513Z","iopub.execute_input":"2024-10-25T05:42:04.932627Z","iopub.status.idle":"2024-10-25T05:42:04.949113Z","shell.execute_reply.started":"2024-10-25T05:42:04.932574Z","shell.execute_reply":"2024-10-25T05:42:04.948050Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:42:22.014605Z","iopub.execute_input":"2024-10-25T05:42:22.015272Z","iopub.status.idle":"2024-10-25T05:42:22.021759Z","shell.execute_reply.started":"2024-10-25T05:42:22.015229Z","shell.execute_reply":"2024-10-25T05:42:22.020650Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"metadata_path_train = os.path.join(DATA_DIR, 'ISIC_2019_Training_Metadata.csv')\nmetadata_path_test = os.path.join(DATA_DIR, '/kaggle/input/isic2019/ISIC_2019_Test_Metadata.csv')\nimage_dir = os.path.join(DATA_DIR, '/kaggle/input/isic2019/ISIC_2019_Training_Input/ISIC_2019_Training_Input')\n\n# Load metadata into DataFrames\nmetadata_train = pd.read_csv(metadata_path_train)\n\ntrain_size = int(0.8 * len(metadata_train))\nval_size = len(metadata_train) - train_size\n\n# Define the path to your ground truth CSV file\nground_truth_csv = os.path.join(DATA_DIR, '/kaggle/input/isic2019/ISIC_2019_Training_GroundTruth.csv')  # Adjust to your actual CSV path\n\n# Create Dataset and DataLoader for training with the ground truth CSV\ntrain_dataset = ISIC2019Dataset(\n    image_dir=image_dir,\n    metadata=metadata_train,\n    ground_truth_csv=ground_truth_csv,\n    transform=data_transforms['train'],\n    is_train=True\n)\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Check the validation dataset size\nprint(f\"Validation dataset size: {len(val_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:46:13.181628Z","iopub.execute_input":"2024-10-25T05:46:13.182051Z","iopub.status.idle":"2024-10-25T05:46:37.434096Z","shell.execute_reply.started":"2024-10-25T05:46:13.182012Z","shell.execute_reply":"2024-10-25T05:46:37.433136Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Validation dataset size: 5067\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Training dataset size: {len(train_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:47:25.155935Z","iopub.execute_input":"2024-10-25T05:47:25.156901Z","iopub.status.idle":"2024-10-25T05:47:25.161585Z","shell.execute_reply.started":"2024-10-25T05:47:25.156858Z","shell.execute_reply":"2024-10-25T05:47:25.160642Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training dataset size: 20264\n","output_type":"stream"}]},{"cell_type":"code","source":"class MoCo(nn.Module):\n    def __init__(self, base_encoder, projection_dim=128, num_classes=6):\n        super(MoCo, self).__init__()\n        self.base_encoder = base_encoder\n        self.projection_head = nn.Sequential(\n            nn.Linear(2048, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.base_encoder(x)\n        features = torch.flatten(features, start_dim=1)\n        projections = self.projection_head(features)\n        return projections\n\n# Load ResNet50 and modify it\nresnet_encoder = models.resnet50(pretrained=True)\nfor param in resnet_encoder.parameters():\n    param.requires_grad = False\nfor param in resnet_encoder.layer4.parameters():\n    param.requires_grad = True\n\nresnet_encoder = nn.Sequential(*list(resnet_encoder.children())[:-1])\n\nmoco_model = MoCo(resnet_encoder)\noptimizer = optim.Adam(moco_model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T05:47:30.624279Z","iopub.execute_input":"2024-10-25T05:47:30.625165Z","iopub.status.idle":"2024-10-25T05:47:31.899030Z","shell.execute_reply.started":"2024-10-25T05:47:30.625121Z","shell.execute_reply":"2024-10-25T05:47:31.898251Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 164MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score\nfrom sklearn.metrics import balanced_accuracy_score\n\ndef train_and_evaluate_isic(model, train_loader, val_loader, optimizer, device, num_epochs=10, patience=5):\n    # Wrap the model in DataParallel if multiple GPUs are available\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n\n    train_losses, val_losses = [], []\n    train_accuracies, val_accuracies = [], []\n    train_bacc, val_bacc = [], []\n    train_precisions, train_recalls, train_f1_scores = [], [], []\n    val_precisions, val_recalls, val_f1_scores = [], [], []\n\n    best_val_loss = float('inf')\n    epochs_without_improvement = 0\n\n    for epoch in range(num_epochs):\n        # ---- TRAINING PHASE ----\n        model.train()\n        total_train_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        y_true_train = []\n        y_pred_train = []\n\n        for x, labels in train_loader:\n            x, labels = x.to(device), labels.to(device)\n            if labels.ndimension() > 1:\n                labels = torch.argmax(labels, dim=1)\n\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct_train += (predicted == labels).sum().item()\n            total_train += labels.size(0)\n\n            y_true_train.extend(labels.cpu().numpy())\n            y_pred_train.extend(predicted.cpu().numpy())\n\n        train_loss = total_train_loss / len(train_loader)\n        train_accuracy = correct_train / total_train\n        train_bacc_score = balanced_accuracy_score(y_true_train, y_pred_train)\n        train_precision = precision_score(y_true_train, y_pred_train, average='macro')\n        train_recall = recall_score(y_true_train, y_pred_train, average='macro')\n        train_f1 = f1_score(y_true_train, y_pred_train, average='macro')\n\n        train_losses.append(train_loss)\n        train_accuracies.append(train_accuracy)\n        train_bacc.append(train_bacc_score)\n        train_precisions.append(train_precision)\n        train_recalls.append(train_recall)\n        train_f1_scores.append(train_f1)\n\n        # ---- VALIDATION PHASE ----\n        model.eval()\n        total_val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n        y_true_val = []\n        y_pred_val = []\n\n        with torch.no_grad():\n            for x, labels in val_loader:\n                x, labels = x.to(device), labels.to(device)\n                if labels.ndimension() > 1:\n                    labels = torch.argmax(labels, dim=1)\n\n                outputs = model(x)\n                loss = criterion(outputs, labels)\n                total_val_loss += loss.item()\n\n                _, predicted = torch.max(outputs, 1)\n                correct_val += (predicted == labels).sum().item()\n                total_val += labels.size(0)\n\n                y_true_val.extend(labels.cpu().numpy())\n                y_pred_val.extend(predicted.cpu().numpy())\n\n        val_loss = total_val_loss / len(val_loader)\n        val_accuracy = correct_val / total_val\n        val_bacc_score = balanced_accuracy_score(y_true_val, y_pred_val)\n        val_precision = precision_score(y_true_val, y_pred_val, average='macro')\n        val_recall = recall_score(y_true_val, y_pred_val, average='macro')\n        val_f1 = f1_score(y_true_val, y_pred_val, average='macro')\n\n        val_losses.append(val_loss)\n        val_accuracies.append(val_accuracy)\n        val_bacc.append(val_bacc_score)\n        val_precisions.append(val_precision)\n        val_recalls.append(val_recall)\n        val_f1_scores.append(val_f1)\n\n        print(f'Epoch {epoch + 1}/{num_epochs} - '\n              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train BAcc: {train_bacc_score:.4f}, '\n              f'Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f} - '\n              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val BAcc: {val_bacc_score:.4f}, '\n              f'Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}')\n\n        # Early stopping logic\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            epochs_without_improvement = 0\n        else:\n            epochs_without_improvement += 1\n\n        if epochs_without_improvement >= patience:\n            print(f\"Early stopping after {epoch + 1} epochs\")\n            break\n\n    # After training, calculate and print classification report for training and validation sets\n    print(\"\\nClassification Report for Training Set:\")\n    print(classification_report(y_true_train, y_pred_train))\n\n    print(\"\\nClassification Report for Validation Set:\")\n    print(classification_report(y_true_val, y_pred_val))\n\n    return (train_losses, val_losses, train_accuracies, val_accuracies, train_bacc, val_bacc,\n            train_precisions, train_recalls, train_f1_scores,\n            val_precisions, val_recalls, val_f1_scores)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T06:57:39.649172Z","iopub.execute_input":"2024-10-25T06:57:39.649979Z","iopub.status.idle":"2024-10-25T06:57:39.672363Z","shell.execute_reply.started":"2024-10-25T06:57:39.649933Z","shell.execute_reply":"2024-10-25T06:57:39.671408Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Start training and evaluation\ntrain_losses, val_losses, train_accuracies, val_accuracies, train_bacc, val_bacc, auc_scores = train_and_evaluate_isic(\n    moco_model, train_loader, val_loader, optimizer, device, num_epochs=20, patience=5\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T06:26:43.923625Z","iopub.execute_input":"2024-10-25T06:26:43.924442Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10 - Train Loss: 0.5298, Train Acc: 0.7981, Train BAcc: 0.6767, Precision: 0.7370, Recall: 0.6767, F1: 0.7016 - Val Loss: 0.6481, Val Acc: 0.7630, Val BAcc: 0.6395, Precision: 0.6212, Recall: 0.6395, F1: 0.6182\nEpoch 2/10 - Train Loss: 0.5177, Train Acc: 0.8042, Train BAcc: 0.6861, Precision: 0.7417, Recall: 0.6861, F1: 0.7094 - Val Loss: 0.6479, Val Acc: 0.7648, Val BAcc: 0.6019, Precision: 0.6754, Recall: 0.6019, F1: 0.6261\nEpoch 3/10 - Train Loss: 0.4993, Train Acc: 0.8108, Train BAcc: 0.6992, Precision: 0.7496, Recall: 0.6992, F1: 0.7206 - Val Loss: 0.6579, Val Acc: 0.7687, Val BAcc: 0.5807, Precision: 0.6891, Recall: 0.5807, F1: 0.6136\nEpoch 4/10 - Train Loss: 0.4890, Train Acc: 0.8129, Train BAcc: 0.7077, Precision: 0.7613, Recall: 0.7077, F1: 0.7308 - Val Loss: 0.6453, Val Acc: 0.7717, Val BAcc: 0.6561, Precision: 0.6998, Recall: 0.6561, F1: 0.6731\nEpoch 5/10 - Train Loss: 0.4667, Train Acc: 0.8227, Train BAcc: 0.7203, Precision: 0.7638, Recall: 0.7203, F1: 0.7395 - Val Loss: 0.6162, Val Acc: 0.7823, Val BAcc: 0.6470, Precision: 0.7079, Recall: 0.6470, F1: 0.6690\nEpoch 6/10 - Train Loss: 0.4472, Train Acc: 0.8299, Train BAcc: 0.7357, Precision: 0.7849, Recall: 0.7357, F1: 0.7573 - Val Loss: 0.6466, Val Acc: 0.7772, Val BAcc: 0.6296, Precision: 0.7059, Recall: 0.6296, F1: 0.6536\nEpoch 7/10 - Train Loss: 0.4323, Train Acc: 0.8360, Train BAcc: 0.7551, Precision: 0.7923, Recall: 0.7551, F1: 0.7719 - Val Loss: 0.6185, Val Acc: 0.7863, Val BAcc: 0.6415, Precision: 0.7186, Recall: 0.6415, F1: 0.6667\nEpoch 8/10 - Train Loss: 0.4167, Train Acc: 0.8453, Train BAcc: 0.7562, Precision: 0.8048, Recall: 0.7562, F1: 0.7777 - Val Loss: 0.6327, Val Acc: 0.7782, Val BAcc: 0.6267, Precision: 0.7082, Recall: 0.6267, F1: 0.6533\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nimport numpy as np\n\n# Convert lists to numpy arrays for easier manipulation\ny_true_val = np.array(y_true_val)\ny_pred_val = np.array(y_pred_val)\n\n# Get unique classes\nclasses = np.unique(y_true_val)\n\n# Initialize dictionaries to store metrics\nmetrics = {}\n\nfor cls in classes:\n    # Get binary labels for the current class\n    y_true_binary = (y_true_val == cls).astype(int)\n    y_pred_binary = (y_pred_val == cls).astype(int)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_true_binary, y_pred_binary)\n    precision = precision_score(y_true_binary, y_pred_binary)\n    recall = recall_score(y_true_binary, y_pred_binary)\n    f1 = f1_score(y_true_binary, y_pred_binary)\n\n    # Store metrics in the dictionary\n    metrics[cls] = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1\n    }\n\n# Print metrics for each class\nfor cls, m in metrics.items():\n    print(f\"Class {cls}:\")\n    print(f\"  Accuracy: {m['accuracy']:.4f}\")\n    print(f\"  Precision: {m['precision']:.4f}\")\n    print(f\"  Recall: {m['recall']:.4f}\")\n    print(f\"  F1 Score: {m['f1_score']:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T06:56:54.510902Z","iopub.execute_input":"2024-10-25T06:56:54.511223Z","iopub.status.idle":"2024-10-25T06:56:54.554070Z","shell.execute_reply.started":"2024-10-25T06:56:54.511188Z","shell.execute_reply":"2024-10-25T06:56:54.552846Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert lists to numpy arrays for easier manipulation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m y_true_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43my_true_val\u001b[49m)\n\u001b[1;32m      6\u001b[0m y_pred_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_pred_val)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get unique classes\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'y_true_val' is not defined"],"ename":"NameError","evalue":"name 'y_true_val' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}